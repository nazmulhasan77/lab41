import tensorflow as tf
print(tf.__version__)


------------

FCNN

from tensorflow.keras.layers import Input,Dense
from tensorflow.keras import Model
inputs = Input((2,)) #vector , not matrix
h1 = Dense(1,activation='relu')(inputs)
h2 = Dense(3,activation='relu')(h1)
h3 = Dense(2,activation='relu')(h2)
outputs = Dense(1, activation='softmax')(h3)
model = Model(inputs, outputs)
model.summary(show_trainable=True)

-----------

CNN MNIST

#Importing Libraries
import tensorflow as tf
from keras.datasets import mnist
from keras.layers import Input, Dense, Flatten,Conv2D, MaxPooling2D, Dropout
from keras.models import Model
import matplotlib.pyplot as plt
import numpy as np
# Load dataset
(trainX, trainY), (testX, testY) = mnist.load_data()
# 2. Normalize and reshape data
trainX = trainX.astype("float32") / 255.0
testX = testX.astype("float32") / 255.0

# CNN expects (height, width, channels)
trainX = trainX[..., tf.newaxis]  
testX = testX[..., tf.newaxis]

#CNN Model Create
inputs = Input((28, 28, 1))  # include channel dimension for grayscale
x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = MaxPooling2D((2, 2))(x)

x = Flatten()(x)
x = Dense(128, activation='relu')(x)
x = Dense(64, activation='relu')(x)
x = Dropout(0.5)(x)
outputs = Dense(10, activation='softmax')(x)
model = Model(inputs, outputs)
# Train Model

model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = model.fit(trainX, trainY, validation_split=0.1, epochs=5, batch_size=32, verbose=1)

# Evaluate on test set
test_loss, test_acc = model.evaluate(testX, testY, verbose=0)
print(f"\n Test Accuracy: {test_acc:.4f}")


# Plot training history
plt.figure(figsize=(8,5))
plt.plot(history.history['accuracy'], label="Train Accuracy")
plt.plot(history.history['val_accuracy'], label="Val Accuracy")
plt.xlabel("No of Epochs")
plt.ylabel("Accuracy")
plt.title("Training Accuracy")
plt.savefig("training_accuracy.png")
plt.legend()
plt.show()

# Predict on test set
predictions = model.predict(testX)
predicted_labels = np.argmax(predictions, axis=1)

# Show 20 test images with predicted labels in a grid
plt.figure(figsize=(12, 8))

for i in range(10):
    plt.subplot(2, 5, i+1)  # 2 rows, 5 columns
    plt.imshow(testX[i], cmap='gray')
    plt.title(f"Pred: {predicted_labels[i]}\nTrue: {testY[i]}")
    plt.axis('off')

plt.tight_layout()
plt.savefig("test_predictions.png")
plt.show()

-----------

#Importing Libraries
import tensorflow as tf
from keras.datasets import mnist
from keras.layers import Input, Dense, Flatten,Conv2D, MaxPooling2D, Dropout
from keras.models import Model
import matplotlib.pyplot as plt
import numpy as np
# Load dataset
(trainX, trainY), (testX, testY) = mnist.load_data()
# 2. Normalize and reshape data
trainX = trainX.astype("float32") / 255.0
testX = testX.astype("float32") / 255.0

# CNN expects (height, width, channels)
trainX = trainX[..., tf.newaxis]  
testX = testX[..., tf.newaxis]

#CNN Model Create
inputs = Input((28, 28, 1))  # include channel dimension for grayscale
x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = MaxPooling2D((2, 2))(x)

x = Flatten()(x)
x = Dense(128, activation='relu')(x)
x = Dense(64, activation='relu')(x)
x = Dropout(0.5)(x)
outputs = Dense(10, activation='softmax')(x)
model = Model(inputs, outputs)
# Train Model

model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = model.fit(trainX, trainY, validation_split=0.1, epochs=5, batch_size=32, verbose=1)

# Evaluate on test set
test_loss, test_acc = model.evaluate(testX, testY, verbose=0)
print(f"\n Test Accuracy: {test_acc:.4f}")


# Plot training history
plt.figure(figsize=(8,5))
plt.plot(history.history['accuracy'], label="Train Accuracy")
plt.plot(history.history['val_accuracy'], label="Val Accuracy")
plt.xlabel("No of Epochs")
plt.ylabel("Accuracy")
plt.title("Training Accuracy")
plt.savefig("training_accuracy.png")
plt.legend()
plt.show()

# Predict on test set
predictions = model.predict(testX)
predicted_labels = np.argmax(predictions, axis=1)

# Show 20 test images with predicted labels in a grid
plt.figure(figsize=(12, 8))

for i in range(10):
    plt.subplot(2, 5, i+1)  # 2 rows, 5 columns
    plt.imshow(testX[i], cmap='gray')
    plt.title(f"Pred: {predicted_labels[i]}\nTrue: {testY[i]}")
    plt.axis('off')

plt.tight_layout()
plt.savefig("test_predictions.png")
plt.show()

-------------

# ========================================
# CIFAR-10 CNN Classifier with Data Augmentation
# and Visualization of Predictions
# ========================================

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import numpy as np
import random

# 1️ Load CIFAR-10 Dataset
(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # normalize

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

print(f"Original training samples: {x_train.shape[0]}")
print(f"Test samples: {x_test.shape[0]}")

# 2️ Data Augmentation
datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    zoom_range=0.1,
    shear_range=0.1
)
datagen.fit(x_train)

# Data augmentation doesn’t *increase* the dataset size on disk,
# but generates new images in memory during each epoch.
# To demonstrate, let’s “generate” one epoch of augmented data:
augmented_images, _ = next(datagen.flow(x_train, y_train, batch_size=len(x_train), shuffle=False))
print(f"Augmented images generated in memory: {augmented_images.shape[0]}")

# 3️ Build CNN Model
model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)),
    layers.Conv2D(32, (3,3), activation='relu', padding='same'),
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.25),

    layers.Conv2D(64, (3,3), activation='relu', padding='same'),
    layers.Conv2D(64, (3,3), activation='relu', padding='same'),
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.25),

    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# 4️ Train Model with Data Augmentation
batch_size = 64
epochs = 20

history = model.fit(
    datagen.flow(x_train, y_train, batch_size=batch_size),
    validation_data=(x_test, y_test),
    steps_per_epoch=len(x_train)//batch_size,
    epochs=epochs,
    verbose=1
)

# 5️ Evaluate Model
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f"\n Test Accuracy: {test_acc*100:.2f}%")

# 6️ Plot Accuracy & Loss Curves
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Val')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Val')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

# 7️ Predict and Visualize Results
num_images = 12
indices = random.sample(range(len(x_test)), num_images)
images = x_test[indices]
true_labels = y_test[indices].flatten()

predictions = model.predict(images)
predicted_labels = np.argmax(predictions, axis=1)

plt.figure(figsize=(12,8))
for i in range(num_images):
    plt.subplot(3,4,i+1)
    plt.imshow(images[i])
    color = 'green' if predicted_labels[i] == true_labels[i] else 'red'
    plt.title(f"P: {class_names[predicted_labels[i]]}\nT: {class_names[true_labels[i]]}", color=color)
    plt.axis('off')
plt.tight_layout()
plt.show()
-------------

 Fully Connected Feedforward Neural Network (FCFNN) for solving the equation f(x) = 5x^2 +10x -2 

from tensorflow.keras.layers import Input, Dense
from tensorflow.keras import Model
import numpy as np
import matplotlib.pyplot as plt

# Polynomial function
def my_polynomial(x):
    return 5 * (x ** 2) + 10 * x - 2

# Generate dataset
def data_process():
    n = 10000
    x = np.random.randint(-100, 100, n)  # Random integers 0-99
    y = my_polynomial(x)
    x = x.reshape(-1, 1)  # Make it 2D for Keras
    y = y.reshape(-1, 1)
    return x, y

# Split dataset
def prepare_train_test_val():
    x, y = data_process()
    total_n = len(x)
    train_n = int(total_n * 0.7)
    val_n = int(total_n * 0.1)

    trainX = x[:train_n]
    trainY = y[:train_n]

    valX = x[train_n:train_n+val_n]
    valY = y[train_n:train_n+val_n]

    testX = x[train_n+val_n:]
    testY = y[train_n+val_n:]

    return (trainX, trainY), (valX, valY), (testX, testY)

# Build the model
def build_model():
    inputs = Input((1,))
    h1 = Dense(8, activation='relu', name='Hidden_Layer_1')(inputs)
    h2 = Dense(16, activation='relu', name='Hidden_Layer_2')(h1)
    h3 = Dense(64, activation='relu', name='Hidden_Layer_3')(h2)
    h4 = Dense(128, activation='relu', name='Hidden_Layer_4')(h3)
    h5 = Dense(32, activation='relu', name='Hidden_Layer_5')(h4)
    h6 = Dense(8, activation='relu', name='Hidden_Layer_6')(h5)
    outputs = Dense(1, name='Output_Layer')(h6)
    
    model = Model(inputs, outputs)
    model.summary(show_trainable=True)
    return model

# Main function
def main():
    model = build_model()
    model.compile(optimizer='adam', loss='mse')

    (trainX, trainY), (valX, valY), (testX, testY) = prepare_train_test_val()

    # Train the model
    history = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=100, batch_size=32, verbose=1)

    # Evaluate on test data
    test_loss = model.evaluate(testX, testY)
    print(f"\nTest Mean Squared Error: {test_loss:.4f}")

    # Predict on test data
    y_pred = model.predict(testX)

    # Plot original vs predicted
    plt.figure(figsize=(8,5))
    plt.scatter(testX, testY, label="Original f(x)", alpha=0.6)
    plt.scatter(testX, y_pred, label="Predicted f(x)", alpha=0.6)
    plt.xlabel("x")
    plt.ylabel("f(x)")
    plt.title("Original vs Predicted Function")
    plt.legend()
    plt.savefig("fx_prediction.png", dpi=300)  # save for LaTeX
    plt.show()

    # Plot training history
    plt.figure(figsize=(8,5))
    plt.plot(history.history['loss'], label="Train Loss")
    plt.plot(history.history['val_loss'], label="Validation Loss")
    plt.xlabel("Epochs")
    plt.ylabel("MSE Loss")
    plt.title("Training History")
    plt.legend()
    plt.savefig("training_history.png", dpi=300)
    plt.show()

if __name__ == '__main__':
    main()


------------

# ==========================================
# Odd vs Even CNN Classifier (from MNIST)
# ==========================================

import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import time, random

# ==============================
# 1️ Load MNIST Dataset
# ==============================
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
#C:\Users\<your_username>\.keras\datasets\mnist.npz
#~/.keras/datasets/mnist.npz

print("Original MNIST:")
print("Train:", x_train.shape, y_train.shape)
print("Test :", x_test.shape, y_test.shape)


# ==============================
# 2️ Convert digits → Odd/Even
# ==============================
# Even -> 0, Odd -> 1
y_train = np.array([0 if d % 2 == 0 else 1 for d in y_train])
y_test  = np.array([0 if d % 2 == 0 else 1 for d in y_test])

class_labels = ['Even', 'Odd']


# ==============================
# 3️ Preprocess Data
# =============================
x_train = x_train.astype('float32') / 255.0
x_test  = x_test.astype('float32') / 255.0

# Add channel dimension
x_train = np.expand_dims(x_train, -1)
x_test  = np.expand_dims(x_test, -1)

# One-hot encoding
y_train_cat = to_categorical(y_train, 2)
y_test_cat  = to_categorical(y_test, 2)

# Train/validation split
x_train, x_val, y_train_cat, y_val_cat = train_test_split(
    x_train, y_train_cat, test_size=0.2, random_state=42
)


# ==============================
# 4️ CNN Model Definition
# ==============================
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,1)),
    tf.keras.layers.MaxPooling2D(2,2),

    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),

    tf.keras.layers.Dense(2, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()


# =============================
# 5️ Training
# ==============================
start_time = time.time()

history = model.fit(
    x_train, y_train_cat,
    epochs=10,
    batch_size=64,
    validation_data=(x_val, y_val_cat)
)

train_time = time.time() - start_time
print(f"\n✅ Training completed in {train_time:.2f} sec")


# ==============================
# 6️ Evaluate
# ==============================
start_test = time.time()

test_loss, test_acc = model.evaluate(x_test, y_test_cat)

test_time = time.time() - start_test

print(f"\n Testing time: {test_time:.2f} sec")
print(f" Test Accuracy: {test_acc*100:.2f}%")


# ==============================
# 7️ Plot Accuracy & Loss
# ==============================
plt.figure(figsize=(10,5))
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Val')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Accuracy Curve')
plt.savefig("odd_even_accuracy.png")
plt.show()


plt.figure(figsize=(10,5))
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Val')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss Curve')
plt.savefig("odd_even_loss.png")
plt.show()


# ==============================
# 8️ Predict 10 Random Images
# ==============================
indices = random.sample(range(len(x_test)), 10)

plt.figure(figsize=(15,8))
correct = 0

for i, idx in enumerate(indices):

    img = x_test[idx]
    true_label = class_labels[y_test[idx]]

    pred = model.predict(np.expand_dims(img,0), verbose=0)
    pred_class = class_labels[np.argmax(pred)]
    conf = np.max(pred)

    if pred_class == true_label:
        correct += 1

    plt.subplot(2,5,i+1)
    plt.imshow(img.squeeze(), cmap='gray')
    plt.title(f"P:{pred_class}\nT:{true_label}\n{conf:.2f}")
    plt.axis('off')

plt.tight_layout()
plt.show()

print(f"\n Correct: {correct}/10 ({correct*10}%)")


# ==============================
# 9️ Save Model
# =============================
model.save("mnist_odd_even_cnn.h5")
print("\n Model saved as 'mnist_odd_even_cnn.h5'")


------------


Transfer Learning



from tensorflow.keras.applications import vgg16
model = vgg16.VGG16()
model.summary(show_trainable = True )

base_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3)) #input_shape=(224, 224, 3)
base_model.summary(show_trainable=True)

inputs = base_model.input
x=base_model.output
x = layers.Flatten()(x)
x = layers.Dense(16, activation='relu')(x)
outputs = layers.Dense(2, activation='softmax')(x)

binary_model = models.Model(inputs=inputs, outputs=outputs)
binary_model.summary(show_trainable=True)

for layer in binary_model.layers[:-2]:
    layer.trainable = False
binary_model.summary(show_trainable=True)
